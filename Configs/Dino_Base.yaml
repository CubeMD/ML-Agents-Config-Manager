behaviors:
    Dino:
        trainer_type: ppo

        hyperparameters:
            # Learning schedule
            batch_size: #64
                opt_values: [16, 32, 64]
            buffer_size: 2048
                #opt_values: [1024, 4096, 8192]
            learning_rate: 0.0003
            learning_rate_schedule: linear

            # PPO-specific hyperparameters
            beta: 0.05
            epsilon: 0.2
            lambd: 0.95
            num_epoch: 3

            # SAC-specific hyperparameters
#              buffer_init_steps: 0
#              tau: 0.005
#              steps_per_update: 10.0
#              save_replay_buffer: false
#              init_entcoef: 0.5
#              reward_signal_steps_per_update: 10.0

        max_steps: 5000000
        time_horizon: 100
        summary_freq: 10000
        keep_checkpoints: 5
        checkpoint_interval: 500000
        threaded: true
#       init_path: null

        # Configuration of the neural network (common to PPO/SAC)
        network_settings:
            vis_encoder_type: simple
            normalize: false
            hidden_units: 64
            num_layers: 3
            # memory
#            memory:
#                sequence_length: 4
#                memory_size: 16

        # behavior cloning
#            behavioral_cloning:
#              demo_path: Project/Assets/ML-Agents/Examples/Pyramids/Demos/ExpertPyramid.demo
#              strength: 0.5
#              steps: 150000
#              batch_size: 512
#              num_epoch: 3
#              samples_per_update: 0

        reward_signals:
            # environment reward (default)
            extrinsic:
                strength: 1.0
                gamma: 0.99

            # curiosity module
#            curiosity:
#                strength: 0.02
#                gamma: 0.99
#                encoding_size: 512
#                learning_rate: 3.0e-4

            # GAIL
#              gail:
#                strength: 0.01
#                gamma: 0.99
#                encoding_size: 128
#                demo_path: Project/Assets/ML-Agents/Examples/Pyramids/Demos/ExpertPyramid.demo
#                learning_rate: 3.0e-4
#                use_actions: false
#                use_vail: false

        # self-play
#        self_play:
#            window: 20 # 10
#            play_against_latest_model_ratio: 0.5
#            save_steps: 15000 # 50000
#            swap_steps: 500 # 2000
#            team_change: 25000 # 100000

        # use TensorFlow backend

env_settings:
    env_path: C:\Users\CubeMD\Desktop\Builds\Dino\Dino
        #opt_values: [C:\Users\CubeMD\Desktop\Builds\Dino\Dino, C:\Users\CubeMD\Desktop\Builds\DinoContinuous\Dino]
#    env_args: null
#    base_port: 5005
    num_envs: 3
        #opt_values: [1, 2, 3]
#    seed:
#        opt_values: [1, 42]

engine_settings:
#    width: 84
#    height: 84
#    quality_level: 5
    time_scale: 20
        #opt_values: [1, 10, 20]
#    target_frame_rate: 60
#    capture_frame_rate: 60
#    no_graphics: false

checkpoint_settings:
    run_id: dino
    #initialize_from: obj20_250b_30kbuf_6
    #load_model: false
    #resume: true
    #force: false
    #train_model: false
    #inference: false

debug: true

torch_settings:
    device: cuda #"none", "cpu", "cuda", or "cuda:0"

environment_parameters:
    decision_frequency:
        opt_values: [10, 30, 50, 100]
    environments_per_unity_process: 20
        #opt_values: [1, 5, 10, 20]
    multi_scene: 0