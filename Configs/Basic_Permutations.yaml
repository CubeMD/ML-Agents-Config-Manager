behaviors:
    Basic:
        trainer_type: ppo

        hyperparameters:
            # Learning schedule
            batch_size:
                opt_values: [16, 64]
            buffer_size:
                opt_values: [256, 1024]
            learning_rate:
                opt_values: [0.0001, 0.0003]
            learning_rate_schedule: linear



            # PPO-specific hyperparameters
            beta: 0.015
            epsilon: 0.2
            lambd: 0.95
            num_epoch:
                opt_values: [1, 3]

            # SAC-specific hyperparameters
#              buffer_init_steps: 0
#              tau: 0.005
#              steps_per_update: 10.0
#              save_replay_buffer: false
#              init_entcoef: 0.5
#              reward_signal_steps_per_update: 10.0

        max_steps: 50000
        time_horizon: 150
        summary_freq: 5000
        keep_checkpoints: 5
        checkpoint_interval: 500000
        threaded: true
#       init_path: null

        # Configuration of the neural network (common to PPO/SAC)
        network_settings:
            vis_encoder_type: simple
            normalize: false
            hidden_units: 30
            num_layers: 1
            # memory
#            memory:
#                sequence_length: 4
#                memory_size: 16

        # behavior cloning
#            behavioral_cloning:
#              demo_path: Project/Assets/ML-Agents/Examples/Pyramids/Demos/ExpertPyramid.demo
#              strength: 0.5
#              steps: 150000
#              batch_size: 512
#              num_epoch: 3
#              samples_per_update: 0

        reward_signals:
            # environment reward (default)
            extrinsic:
                strength: 1.0
                gamma: 0.99

            # curiosity module
#            curiosity:
#                strength: 0.02
#                gamma: 0.99
#                encoding_size: 512
#                learning_rate: 3.0e-4

            # GAIL
#              gail:
#                strength: 0.01
#                gamma: 0.99
#                encoding_size: 128
#                demo_path: Project/Assets/ML-Agents/Examples/Pyramids/Demos/ExpertPyramid.demo
#                learning_rate: 3.0e-4
#                use_actions: false
#                use_vail: false

        # self-play
#        self_play:
#            window: 20 # 10
#            play_against_latest_model_ratio: 0.5
#            save_steps: 15000 # 50000
#            swap_steps: 500 # 2000
#            team_change: 25000 # 100000

        # use TensorFlow backend
#            framework: tensorflow

env_settings:
    env_path: BasicBuild\UnityEnvironment
#    env_args: null
#    base_port: 5005
    num_envs: 5
#    seed: -1

#engine_settings:
#    width: 84
#    height: 84
#    quality_level: 5
#    time_scale: 20
#    target_frame_rate: 60
#    capture_frame_rate: 60
#    no_graphics: false

#checkpoint_settings:
#    run_id: testOfSearch
    #initialize_from: obj20_250b_30kbuf_6
    #load_model: false
    #resume: true
    #force: false
    #train_model: false
    #inference: false

debug: true

torch_settings:
    device: cpu #"none", "cpu", "cuda", or "cuda:0"
