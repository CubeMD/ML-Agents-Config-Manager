behaviors:
    Dino:
        trainer_type: sac

        hyperparameters:
            # Learning schedule
            batch_size: 128
                #opt_values: [64, 128, 256]
            buffer_size: 1000000
                #opt_values: [512, 2048]
            learning_rate: 0.0003
            learning_rate_schedule: linear

#            # PPO-specific hyperparameters
#            beta: 0.05
#            epsilon: 0.2
#            lambd: 0.95
#            num_epoch: 2
#                #opt_values: [2, 4]

#            SAC-specific hyperparameters
            buffer_init_steps: 5000
            tau: 0.005
            steps_per_update: #10.0
                opt_values: [1, 3, 5, 7, 10]
            save_replay_buffer: false
            init_entcoef: 0.5
            reward_signal_steps_per_update: #10.0
                opt_values: [1, 3, 5, 7, 10]

        max_steps: 3000000
        time_horizon: 10
        summary_freq: 10000
        keep_checkpoints: 5
        checkpoint_interval: 500000
        threaded: true
#       init_path: null

        # Configuration of the neural network (common to PPO/SAC)
        network_settings:
            vis_encoder_type: simple
            normalize: false
            hidden_units: 64
#                opt_values: [48, 64, 96, 128]
            num_layers: 3
#                opt_values: [2, 3, 4, 5]
            # memory
#            memory:
#                sequence_length: 4
#                memory_size: 16

        # behavior cloning
#            behavioral_cloning:
#              demo_path: Project/Assets/ML-Agents/Examples/Pyramids/Demos/ExpertPyramid.demo
#              strength: 0.5
#              steps: 150000
#              batch_size: 512
#              num_epoch: 3
#              samples_per_update: 0

        reward_signals:
            # environment reward (default)
            extrinsic:
                strength: 1.0
                gamma: 0.99

            # curiosity module
#            curiosity:
#                strength: 0.02
#                gamma: 0.99
#                encoding_size: 512
#                learning_rate: 3.0e-4

            # GAIL
#              gail:
#                strength: 0.01
#                gamma: 0.99
#                encoding_size: 128
#                demo_path: Project/Assets/ML-Agents/Examples/Pyramids/Demos/ExpertPyramid.demo
#                learning_rate: 3.0e-4
#                use_actions: false
#                use_vail: false

        # self-play
#        self_play:
#            window: 20 # 10
#            play_against_latest_model_ratio: 0.5
#            save_steps: 15000 # 50000
#            swap_steps: 500 # 2000
#            team_change: 25000 # 100000

        # use TensorFlow backend

env_settings:
    env_path: C:\Users\marke\Builds\DinoContinuous\Dino
#    env_args: null
#    base_port: 5005
    num_envs: 3
        #opt_values: [1, 2, 3]
#    seed: #-1
#        opt_values: [-1, -1]

engine_settings:
#    width: 84
#    height: 84
#    quality_level: 5
    time_scale: 30
        #opt_values: [1, 10, 20]
#    target_frame_rate: 60
#    capture_frame_rate: 60
#    no_graphics: false

checkpoint_settings:
    run_id: dino-continuous-50-sac-epochs
    #initialize_from: obj20_250b_30kbuf_6
    #load_model: false
    #resume: true
    #force: false
    #train_model: false
    #inference: false

debug: true

torch_settings:
    device: cuda #"none", "cpu", "cuda", or "cuda:0"

environment_parameters:
    decision_period: 50
        #opt_values: [10, 20, 30, 40, 50, 100]
    environments_per_unity_process: 15
        #opt_values: [1, 5, 10, 20]
    multi_scene: 0