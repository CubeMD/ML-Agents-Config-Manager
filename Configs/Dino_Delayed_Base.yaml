behaviors:
    Dino:
        trainer_type: ppo

        hyperparameters:
            # Learning schedule
            batch_size: 32
                #opt_values: [64, 128, 256]
            buffer_size: 2048
                #opt_values: [512, 2048]
            learning_rate: 0.0003
            learning_rate_schedule: linear

            # PPO-specific hyperparameters
            beta: 0.01
            epsilon: 0.2
            lambd: 0.95
            num_epoch: 3
                #opt_values: [2, 4]

            # SAC-specific hyperparameters
#              buffer_init_steps: 0
#              tau: 0.005
#              steps_per_update: 10.0
#              save_replay_buffer: false
#              init_entcoef: 0.5
#              reward_signal_steps_per_update: 10.0

        max_steps: 2000000
        time_horizon: 25
        summary_freq: 10000
        keep_checkpoints: 5
        checkpoint_interval: 500000
        threaded: true
#       init_path: null

        # Configuration of the neural network (common to PPO/SAC)
        network_settings:
            vis_encoder_type: simple
                #opt_values: [simple, fully_connected]
            normalize: true
            hidden_units: 64
            num_layers: 2
                #opt_values: [2, 3]
            # memory
#            memory:
#                sequence_length: 4
#                memory_size: 16

        # behavior cloning
#            behavioral_cloning:
#              demo_path: Project/Assets/ML-Agents/Examples/Pyramids/Demos/ExpertPyramid.demo
#              strength: 0.5
#              steps: 150000
#              batch_size: 512
#              num_epoch: 3
#              samples_per_update: 0

        reward_signals:
            # environment reward (default)
            extrinsic:
                strength: 1.0
                gamma: 0.99

            # curiosity module
#            curiosity:
#                strength: 0.02
#                gamma: 0.99
#                encoding_size: 512
#                learning_rate: 3.0e-4

            # GAIL
#              gail:
#                strength: 0.01
#                gamma: 0.99
#                encoding_size: 128
#                demo_path: Project/Assets/ML-Agents/Examples/Pyramids/Demos/ExpertPyramid.demo
#                learning_rate: 3.0e-4
#                use_actions: false
#                use_vail: false

        # self-play
#        self_play:
#            window: 20 # 10
#            play_against_latest_model_ratio: 0.5
#            save_steps: 15000 # 50000
#            swap_steps: 500 # 2000
#            team_change: 25000 # 100000

        # use TensorFlow backend

env_settings:
    env_path: Builds\DelayedDino\Dino
#    env_args: null
#    base_port: 5005
    num_envs: 3
        #opt_values: [1, 2, 3]
    seed: #-1
        opt_values: [-1, -1, -1, -1]

engine_settings:
#    width: 84
#    height: 84
#    quality_level: 5
    time_scale: 30
        #opt_values: [1, 10, 20]
#    target_frame_rate: 60
#    capture_frame_rate: 60
#    no_graphics: false

checkpoint_settings:
    run_id: dino-delayed
    #initialize_from: obj20_250b_30kbuf_6
    #load_model: false
    #resume: true
    #force: false
    #train_model: false
    #inference: false
    #results_dir: results

debug: true

torch_settings:
    device: cuda #"none", "cpu", "cuda", or "cuda:0"

environment_parameters:
    decision_period: #50
        opt_values: [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]
    environments_per_unity_process: 20
        #opt_values: [1, 5, 10, 20]
    multi_scene: 0